"""
–ì–æ–ª–æ–≤–Ω–∏–π –∫–ª–∞—Å –¥–ª—è –æ—Ü—ñ–Ω–∫–∏ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—ñ –≤–µ–±—Å–∞–π—Ç—ñ–≤
"""

from playwright.async_api import async_playwright
from typing import Dict, Any, List
import asyncio
import re

from .metrics.perceptibility import PerceptibilityMetrics
from .metrics.operability import OperabilityMetrics  
from .metrics.understandability import UnderstandabilityMetrics
from .metrics.localization import LocalizationMetrics
from .utils.web_scraper import WebScraper
from .utils.calculator import ScoreCalculator


class AccessibilityEvaluator:
    """–ì–æ–ª–æ–≤–Ω–∏–π –∫–ª–∞—Å –¥–ª—è –æ—Ü—ñ–Ω–∫–∏ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—ñ –≤–µ–±—Å–∞–π—Ç—ñ–≤"""
    
    def __init__(self):
        self.weights = {
            'perceptibility': 0.3,
            'operability': 0.3, 
            'understandability': 0.4,
            'localization': 0.4
        }
        
        self.metric_weights = {
            'alt_text': 0.15,
            'contrast': 0.15,
            'media_accessibility': 0.15,
            'keyboard_navigation': 0.05,
            'structured_navigation': 0.05,
            'instruction_clarity': 0.1,
            'input_assistance': 0.1,
            'error_support': 0.1,
            'localization': 0.15
        }
        
        # –Ü–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–∞—Ü—ñ—è –∞–Ω–∞–ª—ñ–∑–∞—Ç–æ—Ä—ñ–≤ –º–µ—Ç—Ä–∏–∫
        self.perceptibility = PerceptibilityMetrics()
        self.operability = OperabilityMetrics()
        self.understandability = UnderstandabilityMetrics()
        self.localization = LocalizationMetrics()
        
        self.web_scraper = WebScraper()
        self.calculator = ScoreCalculator(self.weights, self.metric_weights)
    
    async def evaluate_accessibility(self, url: str) -> Dict[str, Any]:
        """
        –ì–æ–ª–æ–≤–Ω–∞ —Ñ—É–Ω–∫—Ü—ñ—è –¥–ª—è –æ—Ü—ñ–Ω–∫–∏ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—ñ –≤–µ–±—Å–∞–π—Ç—É
        
        Args:
            url: URL –≤–µ–±—Å–∞–π—Ç—É –¥–ª—è –∞–Ω–∞–ª—ñ–∑—É
            
        Returns:
            –°–ª–æ–≤–Ω–∏–∫ –∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ –∞–Ω–∞–ª—ñ–∑—É
        """
        try:
            # –û—Ç—Ä–∏–º–∞–Ω–Ω—è –¥–∞–Ω–∏—Ö –∑ –≤–µ–±—Å–∞–π—Ç—É
            page_data = await self.web_scraper.scrape_page(url)
            
            # –†–æ–∑—Ä–∞—Ö—É–Ω–æ–∫ –≤—Å—ñ—Ö –º–µ—Ç—Ä–∏–∫
            metrics = await self.calculate_all_metrics(page_data)
            
            # –†–æ–∑—Ä–∞—Ö—É–Ω–æ–∫ –ø—ñ–¥—Å–∫–æ—Ä—ñ–≤
            subscores = self.calculator.calculate_subscores(metrics)
            
            # –§—ñ–Ω–∞–ª—å–Ω–∏–π —Å–∫–æ—Ä
            final_score = self.calculator.calculate_final_score(subscores)
            
            # –ì–µ–Ω–µ—Ä–∞—Ü—ñ—è —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü—ñ–π
            recommendations = self.generate_recommendations(metrics)
            
            return {
                'url': url,
                'metrics': metrics,
                'subscores': subscores,
                'final_score': final_score,
                'recommendations': recommendations,
                'axe_results': page_data.get('axe_results', {}),  # –î–æ–¥–∞—î–º–æ axe_results
                'detailed_analysis': await self._generate_detailed_analysis(page_data),  # –î–æ–¥–∞—î–º–æ –¥–µ—Ç–∞–ª—å–Ω–∏–π –∞–Ω–∞–ª—ñ–∑
                'status': 'success'
            }
            
        except Exception as e:
            return {
                'url': url,
                'error': str(e),
                'status': 'error'
            }
    
    async def calculate_all_metrics(self, page_data: Dict[str, Any]) -> Dict[str, float]:
        """–†–æ–∑—Ä–∞—Ö—É–Ω–æ–∫ –≤—Å—ñ—Ö –º–µ—Ç—Ä–∏–∫ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—ñ"""
        
        metrics = {}
        
        # –ü–µ—Ä—Ü–µ–ø—Ç–∏–≤–Ω—ñ—Å—Ç—å
        metrics.update(await self.perceptibility.calculate_metrics(page_data))
        
        # –ö–µ—Ä–æ–≤–∞–Ω—ñ—Å—Ç—å
        metrics.update(await self.operability.calculate_metrics(page_data))
        
        # –ó—Ä–æ–∑—É–º—ñ–ª—ñ—Å—Ç—å
        metrics.update(await self.understandability.calculate_metrics(page_data))
        
        # –õ–æ–∫–∞–ª—ñ–∑–∞—Ü—ñ—è
        metrics.update(await self.localization.calculate_metrics(page_data))
        
        return metrics
    
    def generate_recommendations(self, metrics: Dict[str, float]) -> List[Dict[str, str]]:
        """–ì–µ–Ω–µ—Ä–∞—Ü—ñ—è —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü—ñ–π –Ω–∞ –æ—Å–Ω–æ–≤—ñ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ñ–≤ –º–µ—Ç—Ä–∏–∫"""
        
        recommendations = []
        
        # –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü—ñ—ó –¥–ª—è –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç—É
        if metrics.get('alt_text', 0) < 0.8:
            recommendations.append({
                'category': '–ü–µ—Ä—Ü–µ–ø—Ç–∏–≤–Ω—ñ—Å—Ç—å',
                'issue': '–ù–µ–¥–æ—Å—Ç–∞—Ç–Ω—å–æ –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç—É',
                'recommendation': '–î–æ–¥–∞–π—Ç–µ –∑–º—ñ—Å—Ç–æ–≤–Ω—ñ alt –∞—Ç—Ä–∏–±—É—Ç–∏ –¥–æ –≤—Å—ñ—Ö –∑–æ–±—Ä–∞–∂–µ–Ω—å',
                'priority': '–í–∏—Å–æ–∫–∏–π',
                'wcag_reference': 'WCAG 1.1.1'
            })
        
        # –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü—ñ—ó –¥–ª—è –∫–æ–Ω—Ç—Ä–∞—Å—Ç—É
        if metrics.get('contrast', 0) < 0.7:
            recommendations.append({
                'category': '–ü–µ—Ä—Ü–µ–ø—Ç–∏–≤–Ω—ñ—Å—Ç—å',
                'issue': '–ù–∏–∑—å–∫–∏–π –∫–æ–Ω—Ç—Ä–∞—Å—Ç —Ç–µ–∫—Å—Ç—É',
                'recommendation': '–ü—ñ–¥–≤–∏—â—Ç–µ –∫–æ–Ω—Ç—Ä–∞—Å—Ç –¥–æ –º—ñ–Ω—ñ–º—É–º 4.5:1 –¥–ª—è –æ—Å–Ω–æ–≤–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç—É',
                'priority': '–í–∏—Å–æ–∫–∏–π',
                'wcag_reference': 'WCAG 1.4.3'
            })
        
        # –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü—ñ—ó –¥–ª—è –∫–ª–∞–≤—ñ–∞—Ç—É—Ä–Ω–æ—ó –Ω–∞–≤—ñ–≥–∞—Ü—ñ—ó
        if metrics.get('keyboard_navigation', 0) < 0.9:
            recommendations.append({
                'category': '–ö–µ—Ä–æ–≤–∞–Ω—ñ—Å—Ç—å',
                'issue': '–ü—Ä–æ–±–ª–µ–º–∏ –∑ –∫–ª–∞–≤—ñ–∞—Ç—É—Ä–Ω–æ—é –Ω–∞–≤—ñ–≥–∞—Ü—ñ—î—é',
                'recommendation': '–ó–∞–±–µ–∑–ø–µ—á—Ç–µ –¥–æ—Å—Ç—É–ø–Ω—ñ—Å—Ç—å –≤—Å—ñ—Ö —ñ–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω–∏—Ö –µ–ª–µ–º–µ–Ω—Ç—ñ–≤ —á–µ—Ä–µ–∑ –∫–ª–∞–≤—ñ–∞—Ç—É—Ä—É',
                'priority': '–í–∏—Å–æ–∫–∏–π',
                'wcag_reference': 'WCAG 2.1.1'
            })
        
        # –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü—ñ—ó –¥–ª—è –∑—Ä–æ–∑—É–º—ñ–ª–æ—Å—Ç—ñ
        if metrics.get('instruction_clarity', 0) < 0.7:
            recommendations.append({
                'category': '–ó—Ä–æ–∑—É–º—ñ–ª—ñ—Å—Ç—å',
                'issue': '–°–∫–ª–∞–¥–Ω—ñ –∞–±–æ –Ω–µ–∑—Ä–æ–∑—É–º—ñ–ª—ñ —ñ–Ω—Å—Ç—Ä—É–∫—Ü—ñ—ó',
                'recommendation': '–°–ø—Ä–æ—Å—Ç—ñ—Ç—å –º–æ–≤—É —ñ–Ω—Å—Ç—Ä—É–∫—Ü—ñ–π —Ç–∞ –∑—Ä–æ–±—ñ—Ç—å —ó—Ö –±—ñ–ª—å—à –∑—Ä–æ–∑—É–º—ñ–ª–∏–º–∏',
                'priority': '–°–µ—Ä–µ–¥–Ω—ñ–π',
                'wcag_reference': 'WCAG 3.1.5'
            })
        
        # –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü—ñ—ó –¥–ª—è –ª–æ–∫–∞–ª—ñ–∑–∞—Ü—ñ—ó
        if metrics.get('localization', 0) < 0.6:
            recommendations.append({
                'category': '–õ–æ–∫–∞–ª—ñ–∑–∞—Ü—ñ—è',
                'issue': '–ù–µ–¥–æ—Å—Ç–∞—Ç–Ω—è –ø—ñ–¥—Ç—Ä–∏–º–∫–∞ –º–æ–≤',
                'recommendation': '–î–æ–¥–∞–π—Ç–µ –ø—ñ–¥—Ç—Ä–∏–º–∫—É —É–∫—Ä–∞—ó–Ω—Å—å–∫–æ—ó —Ç–∞ –∞–Ω–≥–ª—ñ–π—Å—å–∫–æ—ó –º–æ–≤',
                'priority': '–°–µ—Ä–µ–¥–Ω—ñ–π',
                'wcag_reference': 'WCAG 3.1.2'
            })
        
        return recommendations
    
    async def evaluate_html_content(self, html_content: str, base_url: str = "http://localhost", title: str = "HTML Document") -> Dict[str, Any]:
        """
        –û—Ü—ñ–Ω–∫–∞ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—ñ HTML –∫–æ–Ω—Ç–µ–Ω—Ç—É –±–µ–∑ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –∑ URL
        
        Args:
            html_content: HTML –∫–æ–Ω—Ç–µ–Ω—Ç –¥–ª—è –∞–Ω–∞–ª—ñ–∑—É
            base_url: –ë–∞–∑–æ–≤–∏–π URL –¥–ª—è –≤—ñ–¥–Ω–æ—Å–Ω–∏—Ö –ø–æ—Å–∏–ª–∞–Ω—å
            title: –ó–∞–≥–æ–ª–æ–≤–æ–∫ –¥–æ–∫—É–º–µ–Ω—Ç–∞
            
        Returns:
            –°–ª–æ–≤–Ω–∏–∫ –∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ –∞–Ω–∞–ª—ñ–∑—É
        """
        try:
            # –°—Ç–≤–æ—Ä—é—î–º–æ page_data –∑ HTML –∫–æ–Ω—Ç–µ–Ω—Ç—É
            page_data = await self._create_page_data_from_html(html_content, base_url, title)
            
            # –†–æ–∑—Ä–∞—Ö—É–Ω–æ–∫ –≤—Å—ñ—Ö –º–µ—Ç—Ä–∏–∫
            metrics = await self.calculate_all_metrics(page_data)
            
            # –†–æ–∑—Ä–∞—Ö—É–Ω–æ–∫ –ø—ñ–¥—Å–∫–æ—Ä—ñ–≤
            subscores = self.calculator.calculate_subscores(metrics)
            
            # –§—ñ–Ω–∞–ª—å–Ω–∏–π —Å–∫–æ—Ä
            final_score = self.calculator.calculate_final_score(subscores)
            
            # –ì–µ–Ω–µ—Ä–∞—Ü—ñ—è —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü—ñ–π
            recommendations = self.generate_recommendations(metrics)
            
            return {
                'url': f"{base_url} (HTML –∫–æ–Ω—Ç–µ–Ω—Ç)",
                'metrics': metrics,
                'subscores': subscores,
                'final_score': final_score,
                'recommendations': recommendations,
                'detailed_analysis': await self._generate_detailed_analysis(page_data),
                'status': 'success'
            }
            
        except Exception as e:
            return {
                'url': f"{base_url} (HTML –∫–æ–Ω—Ç–µ–Ω—Ç)",
                'error': str(e),
                'status': 'error'
            }
    
    async def _create_page_data_from_html(self, html_content: str, base_url: str, title: str) -> Dict[str, Any]:
        """–°—Ç–≤–æ—Ä–µ–Ω–Ω—è page_data –∑ HTML –∫–æ–Ω—Ç–µ–Ω—Ç—É –¥–ª—è –∞–Ω–∞–ª—ñ–∑—É"""
        
        from playwright.async_api import async_playwright
        
        async with async_playwright() as p:
            browser = await p.chromium.launch(headless=True)
            page = await browser.new_page()
            
            try:
                print(f"üìÑ –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è HTML –∫–æ–Ω—Ç–µ–Ω—Ç—É...")
                
                # –í—Å—Ç–∞–Ω–æ–≤–ª—é—î–º–æ HTML –∫–æ–Ω—Ç–µ–Ω—Ç
                await page.set_content(html_content, wait_until="domcontentloaded")
                
                # –ó–±–∏—Ä–∞—î–º–æ –¥–∞–Ω—ñ –∞–Ω–∞–ª–æ–≥—ñ—á–Ω–æ –¥–æ web_scraper
                print("üîç –ó–±—ñ—Ä —ñ–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω–∏—Ö –µ–ª–µ–º–µ–Ω—Ç—ñ–≤...")
                interactive_elements = await self.web_scraper._get_interactive_elements(page)
                
                print("üìù –ó–±—ñ—Ä —Ç–µ–∫—Å—Ç–æ–≤–∏—Ö –µ–ª–µ–º–µ–Ω—Ç—ñ–≤...")
                text_elements = await self.web_scraper._get_text_elements(page)
                
                print("üé¨ –ó–±—ñ—Ä –º–µ–¥—ñ–∞ –µ–ª–µ–º–µ–Ω—Ç—ñ–≤...")
                media_elements = await self.web_scraper._get_media_elements(page)
                
                print("üìã –ó–±—ñ—Ä —Ñ–æ—Ä–º...")
                form_elements = await self.web_scraper._get_form_elements(page)
                
                print("üé® –ó–±—ñ—Ä —Å—Ç–∏–ª—ñ–≤...")
                computed_styles = await self.web_scraper._get_computed_styles(page)
                
                print("üîç –ó–∞–ø—É—Å–∫ axe-core –∞–Ω–∞–ª—ñ–∑—É...")
                axe_results = await self.web_scraper._run_axe_core(page)
                
                print("‚å®Ô∏è –¢–µ—Å—Ç—É–≤–∞–Ω–Ω—è –∫–ª–∞–≤—ñ–∞—Ç—É—Ä–Ω–æ—ó –Ω–∞–≤—ñ–≥–∞—Ü—ñ—ó...")
                focus_test_results = await self.web_scraper._test_keyboard_focus(page)
                
                page_data = {
                    'url': base_url,
                    'html_content': html_content,
                    'title': title,
                    'page_depth': 0,  # HTML –∫–æ–Ω—Ç–µ–Ω—Ç –Ω–µ –º–∞—î –≥–ª–∏–±–∏–Ω–∏
                    'interactive_elements': interactive_elements,
                    'text_elements': text_elements,
                    'media_elements': media_elements,
                    'form_elements': form_elements,
                    'computed_styles': computed_styles,
                    'axe_results': axe_results,
                    'focus_test_results': focus_test_results  # –î–æ–¥–∞—î–º–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∏ —Ç–µ—Å—Ç—É–≤–∞–Ω–Ω—è —Ñ–æ–∫—É—Å—É
                }
                
                print(f"‚úÖ –ó–±—ñ—Ä –¥–∞–Ω–∏—Ö –∑ HTML –∑–∞–≤–µ—Ä—à–µ–Ω–æ. –ó–Ω–∞–π–¥–µ–Ω–æ:")
                print(f"   üìù –¢–µ–∫—Å—Ç–æ–≤–∏—Ö –µ–ª–µ–º–µ–Ω—Ç—ñ–≤: {len(text_elements)}")
                print(f"   üîó –Ü–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω–∏—Ö –µ–ª–µ–º–µ–Ω—Ç—ñ–≤: {len(interactive_elements)}")
                print(f"   üé¨ –ú–µ–¥—ñ–∞ –µ–ª–µ–º–µ–Ω—Ç—ñ–≤: {len(media_elements)}")
                print(f"   üìã –§–æ—Ä–º: {len(form_elements)}")
                
                return page_data
                
            finally:
                await browser.close()
    
    async def _generate_detailed_analysis(self, page_data: Dict[str, Any]) -> Dict[str, Any]:
        """–ì–µ–Ω–µ—Ä–∞—Ü—ñ—è –¥–µ—Ç–∞–ª—å–Ω–æ–≥–æ –∞–Ω–∞–ª—ñ–∑—É –¥–ª—è UI"""
        
        axe_results = page_data.get('axe_results', {})
        
        detailed_analysis = {
            'alt_text': self._analyze_alt_text_details(axe_results),
            'contrast': self._analyze_contrast_details(axe_results),
            'structured_navigation': self._analyze_headings_details(axe_results),
            'keyboard_navigation': self._analyze_keyboard_details(page_data),  # –ü–µ—Ä–µ–¥–∞—î–º–æ page_data –∑–∞–º—ñ—Å—Ç—å axe_results
            'instruction_clarity': self._analyze_instructions_details(page_data),
            'input_assistance': self._analyze_input_assistance_details(page_data),
            'error_support': self._analyze_error_support_details(page_data),
            'media_accessibility': self._analyze_media_details(page_data),  # –ü–µ—Ä–µ–¥–∞—î–º–æ page_data –∑–∞–º—ñ—Å—Ç—å axe_results
            'localization': self._analyze_localization_details(page_data)
        }
        
        return detailed_analysis
    
    def _analyze_alt_text_details(self, axe_results: Dict[str, Any]) -> Dict[str, Any]:
        """–î–µ—Ç–∞–ª—å–Ω–∏–π –∞–Ω–∞–ª—ñ–∑ alt-text"""
        
        details = {
            'total_images': 0,
            'correct_images': 0,
            'problematic_images': [],
            'correct_images_list': [],
            'score_explanation': ''
        }
        
        # –ê–Ω–∞–ª—ñ–∑—É—î–º–æ image-alt –ø—Ä–∞–≤–∏–ª–æ
        violations = self._get_axe_rule_results(axe_results, 'violations', 'image-alt')
        passes = self._get_axe_rule_results(axe_results, 'passes', 'image-alt')
        
        # –ü—Ä–æ–±–ª–µ–º–Ω—ñ –∑–æ–±—Ä–∞–∂–µ–Ω–Ω—è
        if violations:
            for node in violations.get('nodes', []):
                details['problematic_images'].append({
                    'selector': node.get('target', ['–Ω–µ–≤—ñ–¥–æ–º–æ'])[0] if node.get('target') else '–Ω–µ–≤—ñ–¥–æ–º–æ',
                    'html': node.get('html', ''),
                    'issue': node.get('failureSummary', '–í—ñ–¥—Å—É—Ç–Ω—ñ–π alt –∞—Ç—Ä–∏–±—É—Ç'),
                    'impact': violations.get('impact', 'unknown')
                })
        
        # –ü—Ä–∞–≤–∏–ª—å–Ω—ñ –∑–æ–±—Ä–∞–∂–µ–Ω–Ω—è
        if passes:
            for node in passes.get('nodes', []):
                # –í–∏—Ç—è–≥—É—î–º–æ alt —Ç–µ–∫—Å—Ç –∑ HTML
                html = node.get('html', '')
                import re
                alt_match = re.search(r'alt="([^"]*)"', html) if 'alt=' in html else None
                alt_text = alt_match.group(1) if alt_match else '–ü–æ—Ä–æ–∂–Ω—ñ–π alt=""'
                
                details['correct_images_list'].append({
                    'selector': node.get('target', ['–Ω–µ–≤—ñ–¥–æ–º–æ'])[0] if node.get('target') else '–Ω–µ–≤—ñ–¥–æ–º–æ',
                    'html': html,
                    'alt_text': alt_text
                })
        
        details['total_images'] = len(details['problematic_images']) + len(details['correct_images_list'])
        details['correct_images'] = len(details['correct_images_list'])
        
        if details['total_images'] > 0:
            score = details['correct_images'] / details['total_images']
            details['score_explanation'] = f"–°–∫–æ—Ä: {details['correct_images']}/{details['total_images']} = {score:.3f}"
        else:
            details['score_explanation'] = "–ó–æ–±—Ä–∞–∂–µ–Ω–Ω—è –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ"
        
        return details
    
    def _analyze_contrast_details(self, axe_results: Dict[str, Any]) -> Dict[str, Any]:
        """–î–µ—Ç–∞–ª—å–Ω–∏–π –∞–Ω–∞–ª—ñ–∑ –∫–æ–Ω—Ç—Ä–∞—Å—Ç—É"""
        
        details = {
            'total_elements': 0,
            'correct_elements': 0,
            'problematic_elements': [],
            'correct_elements_list': [],
            'score_explanation': ''
        }
        
        # –ê–Ω–∞–ª—ñ–∑—É—î–º–æ color-contrast –ø—Ä–∞–≤–∏–ª–æ
        violations = self._get_axe_rule_results(axe_results, 'violations', 'color-contrast')
        passes = self._get_axe_rule_results(axe_results, 'passes', 'color-contrast')
        
        # –ü—Ä–æ–±–ª–µ–º–Ω—ñ –µ–ª–µ–º–µ–Ω—Ç–∏
        if violations:
            for node in violations.get('nodes', []):
                # –í–∏—Ç—è–≥—É—î–º–æ —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—é –ø—Ä–æ –∫–æ–Ω—Ç—Ä–∞—Å—Ç –∑ failureSummary
                failure_summary = node.get('failureSummary', '')
                contrast_info = self._extract_contrast_info(failure_summary)
                
                details['problematic_elements'].append({
                    'selector': node.get('target', ['–Ω–µ–≤—ñ–¥–æ–º–æ'])[0] if node.get('target') else '–Ω–µ–≤—ñ–¥–æ–º–æ',
                    'html': node.get('html', ''),
                    'issue': failure_summary,
                    'contrast_ratio': contrast_info.get('actual', '–Ω–µ–≤—ñ–¥–æ–º–æ'),
                    'required_ratio': contrast_info.get('required', '–Ω–µ–≤—ñ–¥–æ–º–æ'),
                    'foreground': contrast_info.get('foreground', '–Ω–µ–≤—ñ–¥–æ–º–æ'),
                    'background': contrast_info.get('background', '–Ω–µ–≤—ñ–¥–æ–º–æ')
                })
        
        # –ü—Ä–∞–≤–∏–ª—å–Ω—ñ –µ–ª–µ–º–µ–Ω—Ç–∏
        if passes:
            for node in passes.get('nodes', []):
                details['correct_elements_list'].append({
                    'selector': node.get('target', ['–Ω–µ–≤—ñ–¥–æ–º–æ'])[0] if node.get('target') else '–Ω–µ–≤—ñ–¥–æ–º–æ',
                    'html': node.get('html', ''),
                    'status': '–ö–æ–Ω—Ç—Ä–∞—Å—Ç –≤—ñ–¥–ø–æ–≤—ñ–¥–∞—î WCAG —Å—Ç–∞–Ω–¥–∞—Ä—Ç–∞–º'
                })
        
        details['total_elements'] = len(details['problematic_elements']) + len(details['correct_elements_list'])
        details['correct_elements'] = len(details['correct_elements_list'])
        
        if details['total_elements'] > 0:
            score = details['correct_elements'] / details['total_elements']
            details['score_explanation'] = f"–°–∫–æ—Ä: {details['correct_elements']}/{details['total_elements']} = {score:.3f}"
        else:
            details['score_explanation'] = "–¢–µ–∫—Å—Ç–æ–≤—ñ –µ–ª–µ–º–µ–Ω—Ç–∏ –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ"
        
        return details
    
    def _extract_contrast_info(self, failure_summary: str) -> Dict[str, str]:
        """–í–∏—Ç—è–≥—É—î —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—é –ø—Ä–æ –∫–æ–Ω—Ç—Ä–∞—Å—Ç –∑ –ø–æ–≤—ñ–¥–æ–º–ª–µ–Ω–Ω—è –ø—Ä–æ –ø–æ–º–∏–ª–∫—É"""
        
        import re
        
        info = {}
        
        # –®—É–∫–∞—î–º–æ –∫–æ–Ω—Ç—Ä–∞—Å—Ç ratio
        ratio_match = re.search(r'contrast of ([\d.]+)', failure_summary)
        if ratio_match:
            info['actual'] = ratio_match.group(1) + ':1'
        
        # –®—É–∫–∞—î–º–æ –Ω–µ–æ–±—Ö—ñ–¥–Ω–∏–π –∫–æ–Ω—Ç—Ä–∞—Å—Ç
        required_match = re.search(r'Expected contrast ratio of ([\d.]+):1', failure_summary)
        if required_match:
            info['required'] = required_match.group(1) + ':1'
        
        # –®—É–∫–∞—î–º–æ –∫–æ–ª—å–æ—Ä–∏
        fg_match = re.search(r'foreground color: (#[a-fA-F0-9]+)', failure_summary)
        if fg_match:
            info['foreground'] = fg_match.group(1)
        
        bg_match = re.search(r'background color: (#[a-fA-F0-9]+)', failure_summary)
        if bg_match:
            info['background'] = bg_match.group(1)
        
        return info
    
    def _analyze_headings_details(self, axe_results: Dict[str, Any]) -> Dict[str, Any]:
        """–î–µ—Ç–∞–ª—å–Ω–∏–π –∞–Ω–∞–ª—ñ–∑ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏ –∑–∞–≥–æ–ª–æ–≤–∫—ñ–≤"""
        
        details = {
            'total_headings': 0,
            'correct_headings': 0,
            'problematic_headings': [],
            'correct_headings_list': [],
            'score_explanation': ''
        }
        
        # –ê–Ω–∞–ª—ñ–∑—É—î–º–æ –ø—Ä–∞–≤–∏–ª–∞ –∑–∞–≥–æ–ª–æ–≤–∫—ñ–≤
        heading_rules = ['heading-order', 'page-has-heading-one', 'empty-heading']
        
        for rule_id in heading_rules:
            violations = self._get_axe_rule_results(axe_results, 'violations', rule_id)
            passes = self._get_axe_rule_results(axe_results, 'passes', rule_id)
            
            # –ü—Ä–æ–±–ª–µ–º–Ω—ñ –∑–∞–≥–æ–ª–æ–≤–∫–∏
            if violations:
                for node in violations.get('nodes', []):
                    details['problematic_headings'].append({
                        'selector': node.get('target', ['–Ω–µ–≤—ñ–¥–æ–º–æ'])[0] if node.get('target') else '–Ω–µ–≤—ñ–¥–æ–º–æ',
                        'html': node.get('html', ''),
                        'rule': rule_id,
                        'issue': node.get('failureSummary', violations.get('description', '–ù–µ–≤—ñ–¥–æ–º–∞ –ø—Ä–æ–±–ª–µ–º–∞'))
                    })
            
            # –ü—Ä–∞–≤–∏–ª—å–Ω—ñ –∑–∞–≥–æ–ª–æ–≤–∫–∏
            if passes:
                for node in passes.get('nodes', []):
                    details['correct_headings_list'].append({
                        'selector': node.get('target', ['–Ω–µ–≤—ñ–¥–æ–º–æ'])[0] if node.get('target') else '–Ω–µ–≤—ñ–¥–æ–º–æ',
                        'html': node.get('html', ''),
                        'rule': rule_id,
                        'status': '–ü—Ä–∞–≤–∏–ª—å–Ω–∞ —Å—Ç—Ä—É–∫—Ç—É—Ä–∞'
                    })
        
        details['total_headings'] = len(details['problematic_headings']) + len(details['correct_headings_list'])
        details['correct_headings'] = len(details['correct_headings_list'])
        
        if details['total_headings'] > 0:
            score = details['correct_headings'] / details['total_headings']
            details['score_explanation'] = f"–°–∫–æ—Ä: {details['correct_headings']}/{details['total_headings']} = {score:.3f}"
        else:
            details['score_explanation'] = "–ó–∞–≥–æ–ª–æ–≤–∫–∏ –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ"
        
        return details
    
    def _analyze_keyboard_details(self, page_data: Dict[str, Any]) -> Dict[str, Any]:
        """–î–µ—Ç–∞–ª—å–Ω–∏–π –∞–Ω–∞–ª—ñ–∑ –∫–ª–∞–≤—ñ–∞—Ç—É—Ä–Ω–æ—ó –Ω–∞–≤—ñ–≥–∞—Ü—ñ—ó –∑ —Ä–µ–∞–ª—å–Ω–∏–º —Ç–µ—Å—Ç—É–≤–∞–Ω–Ω—è–º —Ñ–æ–∫—É—Å—É"""
        
        # –û—Ç—Ä–∏–º—É—î–º–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∏ —Ä–µ–∞–ª—å–Ω–æ–≥–æ —Ç–µ—Å—Ç—É–≤–∞–Ω–Ω—è —Ñ–æ–∫—É—Å—É
        focus_test_results = page_data.get('focus_test_results', [])
        
        details = {
            'total_elements': 0,
            'accessible_elements': 0,
            'problematic_elements': [],
            'accessible_elements_list': [],
            'score_explanation': ''
        }
        
        if not focus_test_results:
            details['score_explanation'] = "–†–µ–∑—É–ª—å—Ç–∞—Ç–∏ —Ç–µ—Å—Ç—É–≤–∞–Ω–Ω—è –∫–ª–∞–≤—ñ–∞—Ç—É—Ä–Ω–æ—ó –Ω–∞–≤—ñ–≥–∞—Ü—ñ—ó –Ω–µ–¥–æ—Å—Ç—É–ø–Ω—ñ"
            return details
        
        # –†–æ–∑–¥—ñ–ª—è—î–º–æ –µ–ª–µ–º–µ–Ω—Ç–∏ –Ω–∞ –¥–æ—Å—Ç—É–ø–Ω—ñ —Ç–∞ –ø—Ä–æ–±–ª–µ–º–Ω—ñ
        for result in focus_test_results:
            element_info = {
                'selector': result.get('selector', '–Ω–µ–≤—ñ–¥–æ–º–æ'),
                'html': result.get('html', ''),
                'tag': result.get('tag', 'unknown'),
                'rule': 'focus-test',  # –ü–æ–∑–Ω–∞—á–∞—î–º–æ —è–∫ —Ä–µ–∑—É–ª—å—Ç–∞—Ç —Ä–µ–∞–ª—å–Ω–æ–≥–æ —Ç–µ—Å—Ç—É–≤–∞–Ω–Ω—è
            }
            
            if result.get('focusable', False):
                element_info['status'] = result.get('focus_reason', '–î–æ—Å—Ç—É–ø–Ω–∏–π –∑ –∫–ª–∞–≤—ñ–∞—Ç—É—Ä–∏')
                details['accessible_elements_list'].append(element_info)
            else:
                element_info['issue'] = result.get('non_focus_reason', '–ù–µ–¥–æ—Å—Ç—É–ø–Ω–∏–π –∑ –∫–ª–∞–≤—ñ–∞—Ç—É—Ä–∏')
                details['problematic_elements'].append(element_info)
        
        details['total_elements'] = len(focus_test_results)
        details['accessible_elements'] = len(details['accessible_elements_list'])
        
        if details['total_elements'] > 0:
            score = details['accessible_elements'] / details['total_elements']
            details['score_explanation'] = f"–°–∫–æ—Ä: {details['accessible_elements']}/{details['total_elements']} = {score:.3f} (—Ä–µ–∞–ª—å–Ω–µ —Ç–µ—Å—Ç—É–≤–∞–Ω–Ω—è —Ñ–æ–∫—É—Å—É)"
        else:
            details['score_explanation'] = "–Ü–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—ñ –µ–ª–µ–º–µ–Ω—Ç–∏ –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ"
        
        return details
    
    def _analyze_instructions_details(self, page_data: Dict[str, Any]) -> Dict[str, Any]:
        """–î–µ—Ç–∞–ª—å–Ω–∏–π –∞–Ω–∞–ª—ñ–∑ –∑—Ä–æ–∑—É–º—ñ–ª–æ—Å—Ç—ñ —ñ–Ω—Å—Ç—Ä—É–∫—Ü—ñ–π"""
        
        html_content = page_data.get('html_content', '')
        
        # –í–∏—Ç—è–≥—É—î–º–æ —Ç—ñ–ª—å–∫–∏ labels –¥–ª—è input –ø–æ–ª—ñ–≤
        from bs4 import BeautifulSoup
        soup = BeautifulSoup(html_content, 'html.parser')
        
        instructions = []
        
        # –®—É–∫–∞—î–º–æ labels –ø–æ–≤'—è–∑–∞–Ω—ñ –∑ input –ø–æ–ª—è–º–∏
        labels = soup.find_all('label')
        for label in labels:
            text = label.get_text().strip()
            if text and len(text) >= 2:  # –ú—ñ–Ω—ñ–º–∞–ª—å–Ω–∞ –¥–æ–≤–∂–∏–Ω–∞ –¥–ª—è label
                instructions.append({
                    'text': text,
                    'element': 'label',
                    'for': label.get('for', '–Ω–µ–≤—ñ–¥–æ–º–æ')
                })
        
        # –¢–∞–∫–æ–∂ —à—É–∫–∞—î–º–æ aria-label —Ç–∞ placeholder –≤ input –ø–æ–ª—è—Ö
        inputs = soup.find_all(['input', 'textarea', 'select'])
        for input_elem in inputs:
            # aria-label
            aria_label = input_elem.get('aria-label')
            if aria_label and aria_label.strip():
                instructions.append({
                    'text': aria_label.strip(),
                    'element': 'aria-label',
                    'for': input_elem.get('id', '–Ω–µ–≤—ñ–¥–æ–º–æ')
                })
            
            # placeholder
            placeholder = input_elem.get('placeholder')
            if placeholder and placeholder.strip():
                instructions.append({
                    'text': placeholder.strip(),
                    'element': 'placeholder',
                    'for': input_elem.get('id', '–Ω–µ–≤—ñ–¥–æ–º–æ')
                })
        
        # –ö–æ–Ω–≤–µ—Ä—Ç—É—î–º–æ –≤ –ø—Ä–æ—Å—Ç–∏–π —Å–ø–∏—Å–æ–∫ —Ç–µ–∫—Å—Ç—ñ–≤ –¥–ª—è —Å—É–º—ñ—Å–Ω–æ—Å—Ç—ñ
        instruction_texts = [instr['text'] for instr in instructions]
        
        clear_instructions = []
        problematic_instructions = []
        
        # –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ —ñ—Å–Ω—É—é—á–∏–π –º–µ—Ç–æ–¥ –¥–ª—è –æ—Ü—ñ–Ω–∫–∏ –∑—Ä–æ–∑—É–º—ñ–ª–æ—Å—Ç—ñ
        from accessibility_evaluator.core.metrics.understandability import UnderstandabilityMetrics
        understandability_metrics = UnderstandabilityMetrics()
        
        for i, instruction_text in enumerate(instruction_texts):
            instruction_obj = instructions[i]
            
            # –°—Ç–≤–æ—Ä—é—î–º–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç –¥–ª—è –æ—Ü—ñ–Ω–∫–∏
            context = {
                'field_type': self._get_field_type_for_instruction(instruction_obj, html_content),
                'field_id': instruction_obj.get('for')
            }
            
            # –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ –º–µ—Ç–æ–¥ –∑ –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º
            is_clear = understandability_metrics._assess_instruction_clarity_with_context(instruction_text, context)
            
            if is_clear:
                clear_instructions.append({
                    'text': instruction_text,
                    'element_type': instruction_obj['element'],
                    'status': '–ó—Ä–æ–∑—É–º—ñ–ª–∞ —ñ–Ω—Å—Ç—Ä—É–∫—Ü—ñ—è'
                })
            else:
                # –ê–Ω–∞–ª—ñ–∑—É—î–º–æ —á–æ–º—É label –Ω–µ–∑—Ä–æ–∑—É–º—ñ–ª–∏–π –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—é—á–∏ —Å—Ç—Ä–æ–≥—ñ –∫—Ä–∏—Ç–µ—Ä—ñ—ó
                issues = self._analyze_instruction_issues_strict(instruction_text)
                
                problematic_instructions.append({
                    'text': instruction_text,
                    'element_type': instruction_obj['element'],
                    'issue': '; '.join(issues) if issues else '–°–∫–ª–∞–¥–Ω–∏–π –¥–ª—è —Ä–æ–∑—É–º—ñ–Ω–Ω—è'
                })
        
        total_instructions = len(instructions)
        clear_count = len(clear_instructions)
        
        if total_instructions > 0:
            score = clear_count / total_instructions
            score_explanation = f"–°–∫–æ—Ä: {clear_count}/{total_instructions} = {score:.3f} (–∞–Ω–∞–ª—ñ–∑ labels –¥–ª—è –ø–æ–ª—ñ–≤ –≤–≤–æ–¥—É)"
        else:
            score_explanation = "Labels –¥–ª—è –ø–æ–ª—ñ–≤ –≤–≤–æ–¥—É –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ"
        
        return {
            'total_instructions': total_instructions,
            'clear_instructions': clear_count,
            'problematic_instructions': problematic_instructions,
            'clear_instructions_list': clear_instructions,
            'score_explanation': score_explanation
        }
    
    def _assess_label_clarity(self, text: str, element_type: str) -> bool:
        """–û—Ü—ñ–Ω–∫–∞ –∑—Ä–æ–∑—É–º—ñ–ª–æ—Å—Ç—ñ label –¥–ª—è –ø–æ–ª—è –≤–≤–æ–¥—É –∑ —Ä–µ–∞–ª—ñ—Å—Ç–∏—á–Ω–∏–º–∏ –∫—Ä–∏—Ç–µ—Ä—ñ—è–º–∏"""
        
        # –ë–∞–∑–æ–≤—ñ –ø–µ—Ä–µ–≤—ñ—Ä–∫–∏
        if len(text.strip()) < 2:
            return False
        
        # –î–ª—è —Ä—ñ–∑–Ω–∏—Ö —Ç–∏–ø—ñ–≤ –µ–ª–µ–º–µ–Ω—Ç—ñ–≤ —Ä—ñ–∑–Ω—ñ –∫—Ä–∏—Ç–µ—Ä—ñ—ó
        if element_type == 'placeholder':
            # Placeholder –º–æ–∂–µ –±—É—Ç–∏ –∫–æ—Ä–æ—Ç—à–∏–º
            return len(text) <= 50 and len(text.split()) <= 8
        
        if element_type == 'aria-label':
            # aria-label –º–∞—î –±—É—Ç–∏ –∫–æ—Ä–æ—Ç–∫–∏–º —Ç–∞ –∑—Ä–æ–∑—É–º—ñ–ª–∏–º
            return len(text) <= 100 and len(text.split()) <= 15
        
        # –î–ª—è –∑–≤–∏—á–∞–π–Ω–∏—Ö labels
        word_count = len(text.split())
        
        # –†–µ–∞–ª—ñ—Å—Ç–∏—á–Ω—ñ –∫—Ä–∏—Ç–µ—Ä—ñ—ó –¥–ª—è labels
        basic_criteria = (
            2 <= len(text) <= 100 and           # –†–æ–∑—É–º–Ω–∞ –¥–æ–≤–∂–∏–Ω–∞
            word_count <= 10                    # –ù–µ –±—ñ–ª—å—à–µ 10 —Å–ª—ñ–≤ –¥–ª—è label
        )
        
        if not basic_criteria:
            return False
        
        # –î–ª—è –∫–æ—Ä–æ—Ç–∫–∏—Ö labels (1-3 —Å–ª–æ–≤–∞) –Ω–µ –ø–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ —Å–∫–ª–∞–¥–Ω—ñ—Å—Ç—å
        if word_count <= 3:
            return True
        
        # –î–ª—è –¥–æ–≤—à–∏—Ö labels –ø–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ —á–∏—Ç–∞–±–µ–ª—å–Ω—ñ—Å—Ç—å –∑ –º'—è–∫—à–∏–º–∏ –∫—Ä–∏—Ç–µ—Ä—ñ—è–º–∏
        try:
            import textstat
            flesch_score = textstat.flesch_reading_ease(text)
            grade_level = textstat.flesch_kincaid_grade(text)
            
            # –ú'—è–∫—à—ñ –∫—Ä–∏—Ç–µ—Ä—ñ—ó –¥–ª—è labels
            readability_ok = (
                flesch_score >= 30 or           # –ó–Ω–∞—á–Ω–æ –º'—è–∫—à–∏–π –∫—Ä–∏—Ç–µ—Ä—ñ–π
                grade_level <= 12               # –î–æ 12 –∫–ª–∞—Å—É –∑–∞–º—ñ—Å—Ç—å 8
            )
            
            return readability_ok
            
        except Exception:
            # –Ø–∫—â–æ –Ω–µ –º–æ–∂–µ–º–æ –æ—Ü—ñ–Ω–∏—Ç–∏ - –≤–≤–∞–∂–∞—î–º–æ –∑—Ä–æ–∑—É–º—ñ–ª–∏–º –¥–ª—è –∫–æ—Ä–æ—Ç–∫–∏—Ö —Ç–µ–∫—Å—Ç—ñ–≤
            return word_count <= 6
    
    def _analyze_label_issues(self, text: str, element_type: str) -> list:
        """–ê–Ω–∞–ª—ñ–∑ –ø—Ä–æ–±–ª–µ–º –∑ label"""
        
        issues = []
        word_count = len(text.split())
        
        # –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ –¥–æ–≤–∂–∏–Ω–∏
        if element_type == 'placeholder' and len(text) > 50:
            issues.append(f"–ó–∞–Ω–∞–¥—Ç–æ –¥–æ–≤–≥–∏–π placeholder ({len(text)} —Å–∏–º–≤–æ–ª—ñ–≤)")
        elif element_type == 'aria-label' and len(text) > 100:
            issues.append(f"–ó–∞–Ω–∞–¥—Ç–æ –¥–æ–≤–≥–∏–π aria-label ({len(text)} —Å–∏–º–≤–æ–ª—ñ–≤)")
        elif element_type == 'label' and len(text) > 100:
            issues.append(f"–ó–∞–Ω–∞–¥—Ç–æ –¥–æ–≤–≥–∏–π label ({len(text)} —Å–∏–º–≤–æ–ª—ñ–≤)")
        
        # –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ –∫—ñ–ª—å–∫–æ—Å—Ç—ñ —Å–ª—ñ–≤
        if element_type == 'placeholder' and word_count > 8:
            issues.append(f"–ó–∞–Ω–∞–¥—Ç–æ –±–∞–≥–∞—Ç–æ —Å–ª—ñ–≤ —É placeholder ({word_count} —Å–ª—ñ–≤)")
        elif element_type == 'aria-label' and word_count > 15:
            issues.append(f"–ó–∞–Ω–∞–¥—Ç–æ –±–∞–≥–∞—Ç–æ —Å–ª—ñ–≤ —É aria-label ({word_count} —Å–ª—ñ–≤)")
        elif element_type == 'label' and word_count > 10:
            issues.append(f"–ó–∞–Ω–∞–¥—Ç–æ –±–∞–≥–∞—Ç–æ —Å–ª—ñ–≤ —É label ({word_count} —Å–ª—ñ–≤)")
        
        # –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ —á–∏—Ç–∞–±–µ–ª—å–Ω–æ—Å—Ç—ñ —Ç—ñ–ª—å–∫–∏ –¥–ª—è –¥–æ–≤—à–∏—Ö —Ç–µ–∫—Å—Ç—ñ–≤
        if word_count > 3:
            try:
                import textstat
                flesch_score = textstat.flesch_reading_ease(text)
                grade_level = textstat.flesch_kincaid_grade(text)
                
                if flesch_score < 30:  # –î—É–∂–µ –º'—è–∫–∏–π –∫—Ä–∏—Ç–µ—Ä—ñ–π
                    issues.append(f"–î—É–∂–µ –Ω–∏–∑—å–∫–∞ —á–∏—Ç–∞–±–µ–ª—å–Ω—ñ—Å—Ç—å (Flesch: {flesch_score:.1f})")
                elif grade_level > 12:  # –î–æ 12 –∫–ª–∞—Å—É
                    issues.append(f"–ó–∞–Ω–∞–¥—Ç–æ —Å–∫–ª–∞–¥–Ω–∏–π (—Ä—ñ–≤–µ–Ω—å: {grade_level:.1f} –∫–ª–∞—Å)")
                    
            except Exception:
                pass
        
        return issues
    
    def _analyze_instruction_issues_strict(self, text: str) -> list:
        """–ê–Ω–∞–ª—ñ–∑ –ø—Ä–æ–±–ª–µ–º –∑ —ñ–Ω—Å—Ç—Ä—É–∫—Ü—ñ—î—é –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—é—á–∏ –∞–¥–∞–ø—Ç–æ–≤–∞–Ω—ñ –∫—Ä–∏—Ç–µ—Ä—ñ—ó –∑ UnderstandabilityMetrics"""
        
        issues = []
        
        # –ë–∞–∑–æ–≤—ñ –ø–µ—Ä–µ–≤—ñ—Ä–∫–∏
        if len(text.strip()) < 2:
            issues.append("–ó–∞–Ω–∞–¥—Ç–æ –∫–æ—Ä–æ—Ç–∫–∏–π —Ç–µ–∫—Å—Ç (–º–µ–Ω—à–µ 2 —Å–∏–º–≤–æ–ª—ñ–≤)")
            return issues
            
        if len(text) > 200:
            issues.append(f"–ó–∞–Ω–∞–¥—Ç–æ –¥–æ–≤–≥–∏–π —Ç–µ–∫—Å—Ç ({len(text)} —Å–∏–º–≤–æ–ª—ñ–≤, –º–∞–∫—Å–∏–º—É–º 200)")
        
        word_count = len(text.split())
        sentence_count = len(re.split(r'[.!?]+', text.strip()))
        
        if word_count > 25:
            issues.append(f"–ó–∞–Ω–∞–¥—Ç–æ –±–∞–≥–∞—Ç–æ —Å–ª—ñ–≤ ({word_count}, –º–∞–∫—Å–∏–º—É–º 25)")
            
        if sentence_count > 3:
            issues.append(f"–ó–∞–Ω–∞–¥—Ç–æ –±–∞–≥–∞—Ç–æ —Ä–µ—á–µ–Ω—å ({sentence_count}, –º–∞–∫—Å–∏–º—É–º 3)")
        
        # –ê–Ω–∞–ª—ñ–∑ –∑–∞–ª–µ–∂–Ω–æ –≤—ñ–¥ –¥–æ–≤–∂–∏–Ω–∏ —Ç–µ–∫—Å—Ç—É
        if word_count <= 3:
            # –î–ª—è –∫–æ—Ä–æ—Ç–∫–∏—Ö —Ç–µ–∫—Å—Ç—ñ–≤ –ø–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ —Å–∫–ª–∞–¥–Ω—ñ —Ç–µ—Ä–º—ñ–Ω–∏
            if not self._is_simple_short_text_evaluator(text):
                issues.append("–ú—ñ—Å—Ç–∏—Ç—å —Å–∫–ª–∞–¥–Ω—ñ —Ç–µ—Ö–Ω—ñ—á–Ω—ñ —Ç–µ—Ä–º—ñ–Ω–∏")
        elif word_count <= 8:
            # –î–ª—è —Å–µ—Ä–µ–¥–Ω—ñ—Ö —Ç–µ–∫—Å—Ç—ñ–≤ –ø–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ –¥–æ–¥–∞—Ç–∫–æ–≤—ñ –∫—Ä–∏—Ç–µ—Ä—ñ—ó
            if not self._is_simple_short_text_evaluator(text):
                issues.append("–ú—ñ—Å—Ç–∏—Ç—å —Å–∫–ª–∞–¥–Ω—ñ —Ç–µ—Ö–Ω—ñ—á–Ω—ñ —Ç–µ—Ä–º—ñ–Ω–∏")
            
            words = text.split()
            avg_word_length = sum(len(word) for word in words) / len(words)
            if avg_word_length > 8:
                issues.append(f"–°–µ—Ä–µ–¥–Ω—è –¥–æ–≤–∂–∏–Ω–∞ —Å–ª—ñ–≤ –∑–∞–Ω–∞–¥—Ç–æ –≤–µ–ª–∏–∫–∞ ({avg_word_length:.1f}, –º–∞–∫—Å–∏–º—É–º 8)")
            
            complex_words = [word for word in words if len(word) > 8]
            if len(complex_words) > 1:
                issues.append(f"–ó–∞–Ω–∞–¥—Ç–æ –±–∞–≥–∞—Ç–æ —Å–∫–ª–∞–¥–Ω–∏—Ö —Å–ª—ñ–≤ ({len(complex_words)}, –º–∞–∫—Å–∏–º—É–º 1)")
        else:
            # –î–ª—è –¥–æ–≤—à–∏—Ö —Ç–µ–∫—Å—Ç—ñ–≤ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ textstat –∑ –º'—è–∫—à–∏–º–∏ –∫—Ä–∏—Ç–µ—Ä—ñ—è–º–∏
            try:
                import textstat
                flesch_score = textstat.flesch_reading_ease(text)
                grade_level = textstat.flesch_kincaid_grade(text)
                ari_score = textstat.automated_readability_index(text)
                
                # –ú'—è–∫—à—ñ –∫—Ä–∏—Ç–µ—Ä—ñ—ó
                if flesch_score < 30:
                    issues.append(f"–î—É–∂–µ –Ω–∏–∑—å–∫–∞ —á–∏—Ç–∞–±–µ–ª—å–Ω—ñ—Å—Ç—å (Flesch: {flesch_score:.1f}, –ø–æ—Ç—Ä—ñ–±–Ω–æ >= 30)")
                    
                if grade_level > 10:
                    issues.append(f"–ó–∞–Ω–∞–¥—Ç–æ —Å–∫–ª–∞–¥–Ω–∏–π —Ä—ñ–≤–µ–Ω—å (Grade: {grade_level:.1f}, –ø–æ—Ç—Ä—ñ–±–Ω–æ <= 10)")
                    
                if ari_score > 10:
                    issues.append(f"–í–∏—Å–æ–∫–∏–π ARI —ñ–Ω–¥–µ–∫—Å ({ari_score:.1f}, –ø–æ—Ç—Ä—ñ–±–Ω–æ <= 10)")
                    
            except Exception:
                issues.append("–ù–µ –≤–¥–∞–ª–æ—Å—è –ø—Ä–æ–∞–Ω–∞–ª—ñ–∑—É–≤–∞—Ç–∏ —á–∏—Ç–∞–±–µ–ª—å–Ω—ñ—Å—Ç—å")
        
        return issues
    
    def _is_simple_short_text_evaluator(self, text: str) -> bool:
        """–ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ –ø—Ä–æ—Å—Ç–∏—Ö –∫–æ—Ä–æ—Ç–∫–∏—Ö —Ç–µ–∫—Å—Ç—ñ–≤ –¥–ª—è evaluator.py"""
        
        # –°–ø–∏—Å–æ–∫ —Å–∫–ª–∞–¥–Ω–∏—Ö/—Ç–µ—Ö–Ω—ñ—á–Ω–∏—Ö —Ç–µ—Ä–º—ñ–Ω—ñ–≤
        complex_terms = [
            '–¥–µ—Å–∫—Ä–∏–ø—Ç–∏–≤–Ω–∏–π', '—ñ–¥–µ–Ω—Ç–∏—Ñ—ñ–∫–∞—Ü—ñ—è', '—É–∑–∞–≥–∞–ª—å–Ω–µ–Ω–∏–π', '—Å—É–± º—î–∫—Ç', '–ø–∞—Ä–∞–º–µ—Ç—Ä',
            '–∫–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ—è', '–∞—É—Ç–µ–Ω—Ç–∏—Ñ—ñ–∫–∞—Ü—ñ—è', '–∞–≤—Ç–æ—Ä–∏–∑–∞—Ü—ñ—è', '–≤–∞–ª—ñ–¥–∞—Ü—ñ—è', '–≤–µ—Ä–∏—Ñ—ñ–∫–∞—Ü—ñ—è',
            '—ñ–Ω—Ç–µ–≥—Ä–∞—Ü—ñ—è', '—ñ–º–ø–ª–µ–º–µ–Ω—Ç–∞—Ü—ñ—è', '–æ–ø—Ç–∏–º—ñ–∑–∞—Ü—ñ—è', '—Å–∏–Ω—Ö—Ä–æ–Ω—ñ–∑–∞—Ü—ñ—è', '–º–æ–¥–∏—Ñ—ñ–∫–∞—Ü—ñ—è'
        ]
        
        text_lower = text.lower()
        
        # –Ø–∫—â–æ –º—ñ—Å—Ç–∏—Ç—å —Å–∫–ª–∞–¥–Ω—ñ —Ç–µ—Ä–º—ñ–Ω–∏ - –Ω–µ–∑—Ä–æ–∑—É–º—ñ–ª–∏–π
        for term in complex_terms:
            if term in text_lower:
                return False
        
        # –Ø–∫—â–æ –¥–æ–≤–∂–∏–Ω–∞ —Å–ª–æ–≤–∞ –±—ñ–ª—å—à–µ 12 —Å–∏–º–≤–æ–ª—ñ–≤ - –º–æ–∂–µ –±—É—Ç–∏ —Å–∫–ª–∞–¥–Ω–∏–º
        words = text.split()
        for word in words:
            if len(word) > 12:
                return False
        
        return True
    
    def _get_field_type_for_instruction(self, instruction_obj: Dict[str, Any], html_content: str) -> str:
        """–í–∏–∑–Ω–∞—á–µ–Ω–Ω—è —Ç–∏–ø—É –ø–æ–ª—è –¥–ª—è —ñ–Ω—Å—Ç—Ä—É–∫—Ü—ñ—ó"""
        
        from bs4 import BeautifulSoup
        soup = BeautifulSoup(html_content, 'html.parser')
        
        element_type = instruction_obj.get('element', '')
        field_id = instruction_obj.get('for')
        
        # –î–ª—è label —à—É–∫–∞—î–º–æ –ø–æ–≤'—è–∑–∞–Ω–µ –ø–æ–ª–µ
        if element_type == 'label' and field_id:
            field = soup.find(id=field_id)
            if field:
                return field.get('type', field.name)
        
        # –î–ª—è placeholder —Ç–∞ aria-label —Ç–∏–ø –≤–∂–µ –≤—ñ–¥–æ–º–∏–π –∑ –∫–æ–Ω—Ç–µ–∫—Å—Ç—É –∑–±–æ—Ä—É
        # –ê–ª–µ —è–∫—â–æ –ø–æ—Ç—Ä—ñ–±–Ω–æ, –º–æ–∂–µ–º–æ –∑–Ω–∞–π—Ç–∏ –µ–ª–µ–º–µ–Ω—Ç –∑–∞ —Ç–µ–∫—Å—Ç–æ–º
        if element_type in ['placeholder', 'aria-label']:
            # –®—É–∫–∞—î–º–æ input –∑ —Ç–∞–∫–∏–º placeholder –∞–±–æ aria-label
            text = instruction_obj.get('text', '')
            
            if element_type == 'placeholder':
                field = soup.find(['input', 'textarea'], placeholder=text)
            else:  # aria-label
                field = soup.find(['input', 'textarea'], attrs={'aria-label': text})
            
            if field:
                return field.get('type', field.name)
        
        return 'unknown'
    
    def _analyze_input_assistance_details(self, page_data: Dict[str, Any]) -> Dict[str, Any]:
        """–î–µ—Ç–∞–ª—å–Ω–∏–π –∞–Ω–∞–ª—ñ–∑ –¥–æ–ø–æ–º–æ–≥–∏ –ø—Ä–∏ –≤–≤–µ–¥–µ–Ω–Ω—ñ"""
        
        html_content = page_data.get('html_content', '')
        
        # –í–∏—Ç—è–≥—É—î–º–æ –ø–æ–ª—è –≤–≤–æ–¥—É –∑ HTML —Å–∞–º–æ—Å—Ç—ñ–π–Ω–æ
        from bs4 import BeautifulSoup
        soup = BeautifulSoup(html_content, 'html.parser')
        
        # –¢–∏–ø–∏ input –ø–æ–ª—ñ–≤, —è–∫—ñ –ø–æ—Ç—Ä–µ–±—É—é—Ç—å –¥–æ–ø–æ–º–æ–≥–∏ –ø—Ä–∏ –≤–≤–µ–¥–µ–Ω–Ω—ñ
        text_input_types = [
            'text', 'email', 'password', 'tel', 'url', 'search', 
            'number', 'date', 'datetime-local', 'month', 'week', 'time'
        ]
        
        # –®—É–∫–∞—î–º–æ —Ç—ñ–ª—å–∫–∏ –∑–≤–∏—á–∞–π–Ω—ñ input –ø–æ–ª—è —Ç–∞ textarea (–Ω–µ select, checkbox, radio)
        input_elements = soup.find_all(['input', 'textarea'])
        
        fields = []
        for element in input_elements:
            # –î–ª—è input –ø–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ —Ç–∏–ø
            if element.name == 'input':
                input_type = element.get('type', 'text').lower()
                # –ü—Ä–æ–ø—É—Å–∫–∞—î–º–æ checkbox, radio, submit, button —Ç–æ—â–æ
                if input_type not in text_input_types:
                    continue
            
            # –î–ª—è textarea –∑–∞–≤–∂–¥–∏ –≤—Ä–∞—Ö–æ–≤—É—î–º–æ
            field_info = {
                'selector': f"{element.name}[type='{element.get('type', 'text')}']" if element.name == 'input' else element.name,
                'html': str(element),
                'type': element.get('type', 'text'),
                'placeholder': element.get('placeholder'),
                'autocomplete': element.get('autocomplete'),
                'aria_label': element.get('aria-label'),
                'aria_describedby': element.get('aria-describedby'),
                'title': element.get('title')
            }
            fields.append(field_info)
        
        assisted_fields = []
        problematic_fields = []
        
        for field in fields:
            has_assistance = (
                field.get('autocomplete') or
                field.get('placeholder') or
                field.get('aria_describedby') or
                field.get('aria_label') or
                field.get('title')
            )
            
            if has_assistance:
                assistance_types = []
                if field.get('placeholder'):
                    assistance_types.append(f"placeholder='{field['placeholder']}'")
                if field.get('autocomplete'):
                    assistance_types.append(f"autocomplete='{field['autocomplete']}'")
                if field.get('aria_label'):
                    assistance_types.append(f"aria-label='{field['aria_label']}'")
                if field.get('title'):
                    assistance_types.append(f"title='{field['title']}'")
                if field.get('aria_describedby'):
                    assistance_types.append(f"aria-describedby='{field['aria_describedby']}'")
                
                assisted_fields.append({
                    'selector': field.get('selector', '–Ω–µ–≤—ñ–¥–æ–º–æ'),
                    'html': field.get('html', ''),
                    'assistance': '; '.join(assistance_types)
                })
            else:
                problematic_fields.append({
                    'selector': field.get('selector', '–Ω–µ–≤—ñ–¥–æ–º–æ'),
                    'html': field.get('html', ''),
                    'type': field.get('type', 'text'),
                    'issue': '–í—ñ–¥—Å—É—Ç–Ω—ñ –ø—ñ–¥–∫–∞–∑–∫–∏ (placeholder, autocomplete, aria-label, title)'
                })
        
        total_fields = len(fields)
        assisted_count = len(assisted_fields)
        
        if total_fields > 0:
            score = assisted_count / total_fields
            score_explanation = f"–°–∫–æ—Ä: {assisted_count}/{total_fields} = {score:.3f} (–∞–Ω–∞–ª—ñ–∑ —Ç—ñ–ª—å–∫–∏ —Ç–µ–∫—Å—Ç–æ–≤–∏—Ö –ø–æ–ª—ñ–≤: input[text,email,password,etc] —Ç–∞ textarea)"
        else:
            score_explanation = "–¢–µ–∫—Å—Ç–æ–≤—ñ –ø–æ–ª—è –≤–≤–æ–¥—É –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ (checkbox, radio, select –Ω–µ –≤—Ä–∞—Ö–æ–≤—É—é—Ç—å—Å—è)"
        
        return {
            'total_fields': total_fields,
            'assisted_fields': assisted_count,
            'problematic_fields': problematic_fields,
            'assisted_fields_list': assisted_fields,
            'score_explanation': score_explanation
        }
    
    def _analyze_error_support_details(self, page_data: Dict[str, Any]) -> Dict[str, Any]:
        """–î–µ—Ç–∞–ª—å–Ω–∏–π –∞–Ω–∞–ª—ñ–∑ –ø—ñ–¥—Ç—Ä–∏–º–∫–∏ –ø–æ–º–∏–ª–æ–∫"""
        
        html_content = page_data.get('html_content', '')
        
        # –í–∏—Ç—è–≥—É—î–º–æ —Ñ–æ—Ä–º–∏ –∑ HTML —Å–∞–º–æ—Å—Ç—ñ–π–Ω–æ
        from bs4 import BeautifulSoup
        soup = BeautifulSoup(html_content, 'html.parser')
        
        # –®—É–∫–∞—î–º–æ –≤—Å—ñ —Ñ–æ—Ä–º–∏
        form_elements = soup.find_all('form')
        
        forms = []
        for form in form_elements:
            # –ê–Ω–∞–ª—ñ–∑—É—î–º–æ —Ñ–æ—Ä–º—É –Ω–∞ –Ω–∞—è–≤–Ω—ñ—Å—Ç—å –º–µ—Ö–∞–Ω—ñ–∑–º—ñ–≤ –æ–±—Ä–æ–±–∫–∏ –ø–æ–º–∏–ª–æ–∫
            has_error_elements = bool(form.find_all(class_=lambda x: x and any(word in x.lower() for word in ['error', 'invalid', 'warning'])))
            has_aria_invalid = bool(form.find_all(attrs={'aria-invalid': True}))
            has_required_fields = bool(form.find_all(attrs={'required': True}))
            has_validation = not form.get('novalidate', False)
            has_role_alert = bool(form.find_all(attrs={'role': 'alert'}))
            
            form_info = {
                'selector': 'form',
                'html': str(form)[:200] + '...' if len(str(form)) > 200 else str(form),
                'has_error_elements': has_error_elements,
                'has_aria_invalid': has_aria_invalid,
                'has_required_fields': has_required_fields,
                'has_validation': has_validation,
                'has_role_alert': has_role_alert
            }
            forms.append(form_info)
        
        supported_forms = []
        problematic_forms = []
        
        for form in forms:
            error_support_features = []
            
            # –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ —Ä—ñ–∑–Ω—ñ —Ç–∏–ø–∏ –ø—ñ–¥—Ç—Ä–∏–º–∫–∏ –ø–æ–º–∏–ª–æ–∫
            if form.get('has_error_elements'):
                error_support_features.append("–ï–ª–µ–º–µ–Ω—Ç–∏ –¥–ª—è –≤—ñ–¥–æ–±—Ä–∞–∂–µ–Ω–Ω—è –ø–æ–º–∏–ª–æ–∫")
            if form.get('has_aria_invalid'):
                error_support_features.append("aria-invalid –∞—Ç—Ä–∏–±—É—Ç–∏")
            if form.get('has_required_fields'):
                error_support_features.append("–û–±–æ–≤'—è–∑–∫–æ–≤—ñ –ø–æ–ª—è (required)")
            if form.get('has_validation'):
                error_support_features.append("HTML5 –≤–∞–ª—ñ–¥–∞—Ü—ñ—è")
            if form.get('has_role_alert'):
                error_support_features.append("role='alert' –¥–ª—è –ø–æ–≤—ñ–¥–æ–º–ª–µ–Ω—å")
            
            if error_support_features:
                supported_forms.append({
                    'selector': form.get('selector', '–Ω–µ–≤—ñ–¥–æ–º–æ'),
                    'html': form.get('html', ''),
                    'features': '; '.join(error_support_features)
                })
            else:
                problematic_forms.append({
                    'selector': form.get('selector', '–Ω–µ–≤—ñ–¥–æ–º–æ'),
                    'html': form.get('html', ''),
                    'issue': '–í—ñ–¥—Å—É—Ç–Ω—è –æ–±—Ä–æ–±–∫–∞ –ø–æ–º–∏–ª–æ–∫ (–Ω–µ–º–∞—î error –µ–ª–µ–º–µ–Ω—Ç—ñ–≤, aria-invalid, –≤–∞–ª—ñ–¥–∞—Ü—ñ—ó)'
                })
        
        total_forms = len(forms)
        supported_count = len(supported_forms)
        
        if total_forms > 0:
            score = supported_count / total_forms
            score_explanation = f"–°–∫–æ—Ä: {supported_count}/{total_forms} = {score:.3f}"
        else:
            score_explanation = "–§–æ—Ä–º–∏ –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ"
        
        return {
            'total_forms': total_forms,
            'supported_forms': supported_count,
            'problematic_forms': problematic_forms,
            'supported_forms_list': supported_forms,
            'score_explanation': score_explanation
        }
    
    def _analyze_media_details(self, page_data: Dict[str, Any]) -> Dict[str, Any]:
        """–î–µ—Ç–∞–ª—å–Ω–∏–π –∞–Ω–∞–ª—ñ–∑ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—ñ –º–µ–¥—ñ–∞ –≤–∫–ª—é—á–Ω–æ –∑ embedded –≤—ñ–¥–µ–æ"""
        
        media_elements = page_data.get('media_elements', [])
        video_elements = [elem for elem in media_elements if elem['type'] in ['video', 'embedded_video']]
        
        details = {
            'total_media': len(video_elements),
            'accessible_media': 0,
            'problematic_media': [],
            'accessible_media_list': [],
            'score_explanation': ''
        }
        
        if not video_elements:
            details['score_explanation'] = "–í—ñ–¥–µ–æ –µ–ª–µ–º–µ–Ω—Ç–∏ –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ"
            return details
        
        for video in video_elements:
            video_type = video.get('type', 'unknown')
            platform = video.get('platform', 'native')
            src = video.get('src') or ''
            
            selector = f"iframe[src*='{platform}']" if video_type == 'embedded_video' else 'video'
            
            video_info = {
                'type': video_type,
                'platform': platform,
                'src': src,
                'title': video.get('title', '–ë–µ–∑ –Ω–∞–∑–≤–∏'),
                'selector': selector,
                'html': f"<{selector} src=\"{src[:50]}...\">" if src and len(src) > 50 else f"<{selector} src=\"{src}\">"
            }
            
            has_accessibility = False
            accessibility_features = []
            
            if video_type == 'video':
                # –ù–∞—Ç–∏–≤–Ω–µ HTML5 –≤—ñ–¥–µ–æ
                tracks = video.get('tracks', [])
                
                for track in tracks:
                    track_kind = track.get('kind', '')
                    if track_kind in ['subtitles', 'captions']:
                        has_accessibility = True
                        accessibility_features.append(f"–°—É–±—Ç–∏—Ç—Ä–∏ ({track_kind})")
                    elif track_kind == 'descriptions':
                        has_accessibility = True
                        accessibility_features.append("–ê—É–¥—ñ–æ–æ–ø–∏—Å–∏")
            
            elif video_type == 'embedded_video':
                # Embedded –≤—ñ–¥–µ–æ
                has_captions = video.get('has_captions', False)
                caption_check_method = video.get('caption_check_method', 'url_params')
                
                if has_captions:
                    has_accessibility = True
                    if caption_check_method == 'youtube_api':
                        accessibility_features.append(f"–°—É–±—Ç–∏—Ç—Ä–∏ –ø—ñ–¥—Ç–≤–µ—Ä–¥–∂–µ–Ω—ñ YouTube API ({platform})")
                    elif caption_check_method == 'enhanced_url_analysis':
                        # –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ —á–∏ —î —è–≤–Ω—ñ –ø–∞—Ä–∞–º–µ—Ç—Ä–∏ —Å—É–±—Ç–∏—Ç—Ä—ñ–≤
                        if any(param in src for param in ['cc_load_policy=1', 'captions=1', 'cc_lang_pref=']):
                            accessibility_features.append(f"–°—É–±—Ç–∏—Ç—Ä–∏ –ø—ñ–¥—Ç–≤–µ—Ä–¥–∂–µ–Ω—ñ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏ URL ({platform})")
                        elif any(param in src for param in ['hl=en', 'hl=uk', 'hl=ru', 'hl=de', 'hl=fr']):
                            accessibility_features.append(f"–ô–º–æ–≤—ñ—Ä–Ω—ñ –∞–≤—Ç–æ—Å—É–±—Ç–∏—Ç—Ä–∏ –∑–∞ –º–æ–≤–Ω–∏–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏ ({platform})")
                        else:
                            accessibility_features.append(f"–ô–º–æ–≤—ñ—Ä–Ω—ñ –∞–≤—Ç–æ–º–∞—Ç–∏—á–Ω—ñ —Å—É–±—Ç–∏—Ç—Ä–∏ YouTube (—Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–µ –≤—ñ–¥–µ–æ)")
                    else:
                        accessibility_features.append(f"–°—É–±—Ç–∏—Ç—Ä–∏ –≤ URL ({platform})")
            
            if has_accessibility:
                video_info['status'] = f"–î–æ—Å—Ç—É–ø–Ω–µ: {', '.join(accessibility_features)}"
                details['accessible_media_list'].append(video_info)
                details['accessible_media'] += 1
            else:
                video_info['issue'] = "–í—ñ–¥—Å—É—Ç–Ω—ñ —Å—É–±—Ç–∏—Ç—Ä–∏ —Ç–∞ –∞—É–¥—ñ–æ–æ–ø–∏—Å–∏"
                details['problematic_media'].append(video_info)
        
        if details['total_media'] > 0:
            score = details['accessible_media'] / details['total_media']
            details['score_explanation'] = f"–°–∫–æ—Ä: {details['accessible_media']}/{details['total_media']} = {score:.3f} (–∞–Ω–∞–ª—ñ–∑ –Ω–∞—Ç–∏–≤–Ω–∏—Ö —Ç–∞ embedded –≤—ñ–¥–µ–æ)"
        else:
            details['score_explanation'] = "–í—ñ–¥–µ–æ –µ–ª–µ–º–µ–Ω—Ç–∏ –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ"
        
        return details
    
    def _analyze_localization_details(self, page_data: Dict[str, Any]) -> Dict[str, Any]:
        """–î–µ—Ç–∞–ª—å–Ω–∏–π –∞–Ω–∞–ª—ñ–∑ –ª–æ–∫–∞–ª—ñ–∑–∞—Ü—ñ—ó"""
        
        html_content = page_data.get('html_content', '')
        url = page_data.get('url', '')
        
        # –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ —ñ—Å–Ω—É—é—á–∏–π –º–µ—Ç–æ–¥ –¥–ª—è –≤–∏–∑–Ω–∞—á–µ–Ω–Ω—è –º–æ–≤
        from accessibility_evaluator.core.metrics.localization import LocalizationMetrics
        localization_metrics = LocalizationMetrics()
        
        detected_languages_set = localization_metrics._detect_available_languages(html_content, url)
        
        # –ö–æ–Ω–≤–µ—Ä—Ç—É—î–º–æ –≤ –¥–µ—Ç–∞–ª—å–Ω—É —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—é
        language_names = {
            'uk': '–£–∫—Ä–∞—ó–Ω—Å—å–∫–∞',
            'en': '–ê–Ω–≥–ª—ñ–π—Å—å–∫–∞', 
            'de': '–ù—ñ–º–µ—Ü—å–∫–∞',
            'fr': '–§—Ä–∞–Ω—Ü—É–∑—å–∫–∞',
            'ru': '–†–æ—Å—ñ–π—Å—å–∫–∞',
            'pl': '–ü–æ–ª—å—Å—å–∫–∞'
        }
        
        detected_languages = []
        for lang_code in detected_languages_set:
            lang_name = language_names.get(lang_code, f'–ú–æ–≤–∞ ({lang_code})')
            weight = localization_metrics.weights.get(lang_code, 0.01)
            detected_languages.append({
                'code': lang_code,
                'name': lang_name,
                'weight': weight
            })
        
        # –í–∏–∑–Ω–∞—á–∞—î–º–æ –≤—ñ–¥—Å—É—Ç–Ω—ñ –≤–∞–∂–ª–∏–≤—ñ –º–æ–≤–∏
        important_languages = ['uk', 'en']
        missing_languages = []
        for lang_code in important_languages:
            if lang_code not in detected_languages_set:
                lang_name = language_names.get(lang_code, f'–ú–æ–≤–∞ ({lang_code})')
                weight = localization_metrics.weights.get(lang_code, 0.01)
                missing_languages.append({
                    'code': lang_code,
                    'name': lang_name,
                    'weight': weight
                })
        
        # –†–æ–∑—Ä–∞—Ö—É–Ω–æ–∫ —Å–∫–æ—Ä—É
        total_score = 0
        for lang in detected_languages:
            total_score += lang['weight']
        
        score_explanation = f"–°–∫–æ—Ä: {total_score:.3f} (–≤–∏—è–≤–ª–µ–Ω–æ {len(detected_languages)} –º–æ–≤)"
        
        return {
            'detected_languages': detected_languages,
            'missing_languages': missing_languages,
            'score_explanation': score_explanation
        }
    
    def _get_axe_rule_results(self, axe_results: Dict[str, Any], result_type: str, rule_id: str) -> Dict[str, Any]:
        """–û—Ç—Ä–∏–º–∞–Ω–Ω—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ñ–≤ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ –ø—Ä–∞–≤–∏–ª–∞ axe-core"""
        
        results = axe_results.get(result_type, [])
        for result in results:
            if result.get('id') == rule_id:
                return result
        return {}